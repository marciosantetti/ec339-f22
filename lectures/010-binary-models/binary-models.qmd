---
title: "Models with Binary Dependent Variables"
author: "*Marcio Santetti*"
date: "EC 339 | Fall 2022"
format:
  pdf:
    pdf-engine: pdflatex
    toc: true
    number-sections: false
    linestretch: "1.10"
    # fontfamily: libertine
    header-includes: |
      \usepackage{fontawesome}
      \usepackage[T1]{fontenc}
      \usepackage{newpxtext,eulerpx}
reference-location: margin
citation-location: margin
---


\newpage

```{r}
#| echo: false
#| message: false
#| warning: false


library(tidyverse)
library(showtext)
library(patchwork)
library(wooldridge)
library(scales)
library(lmtest)


font_add_google("PT Sans Narrow", "jost")

showtext_auto()


my_theme <- theme_gray() +
  theme(text =  element_text(family = "jost"),
        plot.title = element_text(family = "jost"))

theme_set(my_theme)





```


# Introduction

So far, we have applied OLS regression to models whose dependent variables were *quantitative* by definition. However, sometimes our research question involves estimating potential determinants of *qualitative* variables. A few weeks ago, we studied how to use and interpret binary (*dummy*) variables when these were located on the right-hand side of the regression model. Now, we investigate how to properly estimate and interpret models with this kind of variable on the left-hand side, as the explained variable.

In this lecture, we will further investigate three different techniques for the above purpose. First, we will see what happens when we use our traditional OLS estimation for the binary dependent variable case. We will see that this is not an appropriate method, since it does not fully capture the intrinsic limitations of the dependent variable. Then, we move on to non-linear techniques, the Binomial *Logit* and *Probit* models, which take care of the things that OLS cannot support. 




# The Linear Probability Model

In case there were no better techniques to estimate models with *dummy* dependent variables, what would we do? Based on our previous knowledge, we would run OLS regression and see what happens. When this is the case, we call it the **Linear Probability Model** (LPM). But why do we call it by a different name, and not just the usual linear regression model?

Recall that binary variables take on either 1 or 0 values. When located on the left-hand side of an econometric model, our research question needs to change a bit. Since we are now dealing with *discrete choice* topics, the interpretation is no longer "a one-unit increase in $x_i$ will change $y_i$ by $\beta_i$ units," but how changes in the independent variables affect the *likelihood* of success (i.e., the *dummy* variable being equal to 1). This is a similar reasoning as the one we are used to, though adapted to this new kind of dependent variable.

A *Linear Probability Model* is simply a linear-in-parameters equation that aims to explain a binary dependent variable:

$$ D_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_kx_{ki} + u_i $$


where $D_i$ is a *dummy* variable, and the right-hand side elements are those familiar parameter, variable, and error terms.


Let us now take the Expected Value on both sides:

$$ \mathbb{E}(D_i|x_i) = P(D_i = 1 | x_i) = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_kx_{ki}  $$

where $P(D_i = 1|x_i)$ is the probability of success, that is, the probability that the *dummy* variable equals 1 for the $i^{th}$ observation, given the independent regressors. This term is also known as *response probability*.

To measure the change in the response probability from a one-unit change in the $j^{th}$ variable, we use the partial derivatives method we saw a few weeks ago:

$$ \Delta P(D_i = 1 | x_j) = \beta_j  $$



Lastly, the predicted probability of success, i.e., of $D_i$ being equal to 1 is


$$ \hat{D} = \hat{\beta}_0 + \hat{\beta}_1x_1 + \hat{\beta}_2x_2 + ... + \hat{\beta}_kx_k   $$

## Issues with the Linear Probability Model


Using OLS to estimate binary response models is straightforward. However, there are many problems associated with this practice, which will become clear as soon as we look at an example. But before that, a couple of things worth noting:

1. The estimated dependent variable, $\hat{D}_i$, is not bounded by 0 and 1. Even though a dummy variable can only take on 0 or 1 values, as soon as the regression model is estimated by OLS, we can no longer guarantee this fact. Depending on the values of the independent variables ($x_i$) and the estimated $\beta$ coefficients ($\hat{\beta}_i$), the value of $\hat{D}_i$ may go below 0 or above 1, which does not make logical sense, since probabilities must lie between 0 and 1.

2. Usual goodness-of-fit measures, such as the $R^2$ and the adjusted $R^2$ will not reflect an accurate measure of overall fit. For a similar reason as the one for the item above, the nature of these measures do not allow for a precise measurement of model fit. Therefore, LPMs may return negative values for the adjusted $R^2$, even if the coefficients may make practical sense.

Dealing with this latter issue is not a big problem. We will learn how to calculate a better goodness-of-fit measure for these special models. And with respect to the first, we will look at more robust regression techniques for binary dependent variable models. Let us first look at a practical example of a Linar Probability Model.





## An example

Suppose we want to determine the most relevant factors for labor force participation. An individual can either participate ($part = 1$) or not participate ($part = 0$) in the labor force. Thus, we have already defined our dependent variable, and it is a *dummy* variable. As independent variables, we will assume relevant the individual's *age* and its *squared* term, her *income*, years of *education*, the number of young children (*youngkids*), the number of children over 7 years of age (*oldkids*), and whether the individual is a foreigner or not (*foreign*). Our data set contains 872 observations, collected from a health survey conducted in Switzerland in 1981.

Our econometric model looks like this:

$$
part_i = \beta_0 + \beta_1income_i + \beta_2age_i + \beta_3age_i^2 + \beta_4education_i + \beta_5 youngkids_i  \\ + \beta_6oldkids_i + \beta_7foreign_i + u_i
$$

 
```{r}
#| echo: false
#| message: false
#| warning: false
#| results: asis

library(stargazer)
library(AER)

data("SwissLabor")


attach(SwissLabor)


SwissLabor$part <- as.numeric(SwissLabor$participation == 'yes')
SwissLabor$foreign <- as.numeric(SwissLabor$foreign == 'yes')



lpm <- lm(part ~ income + age + I(age^2) + education + youngkids + oldkids + foreign, data=SwissLabor)
logit <- glm(part ~ income + age + I(age^2) + education + youngkids + oldkids + foreign, data=SwissLabor,
             family = binomial(link='logit'))
probit <- glm(part ~ income + age + I(age^2) + education + youngkids + oldkids + foreign, data=SwissLabor,
              family = binomial(link='probit'))


stargazer(lpm, title="A Linear Probability Model", header=FALSE, no.space = TRUE)

```

Table 1 illustrates this regression's estimates. This Linear Probability Model is interpreted in the same way as we are used to, only changing the effect on the dependent variable. Now, we are interpreting the likelihood of success, that is, likelihood of the *dummy* variable being equal to 1. In our case, we are investigating the likelihood of an individual participating in the labor force ($part_i=1$), given our chosen independent variables.


First, let us analyze the coefficients' signs. Coefficients on *income*, $age^2$, *youngkids*, and *oldkids* are negative, meaning that the higher the income, the older, and the more children (regardless of age) an individual is/has, the *less likely* she is to participate in the labor force. Conversely, for the other factors, their positive signs indicate an *increased likelihood* of joining the labor force. 


But what about marginal effects? In other words, how to make sense of the values returned by this regression? First, the table shows us that, except for years of *education*, all covariates are statistically significant at $\alpha = 0.01$. Moreover, the adjusted $R^2$ indicates that the model is responsible for explaining 18.6\% of the predicted likelihood of an individual participating in the labor force. Lastly, on the estimated coefficients. The average age for this data set is close to 40 years old.^[This data set has *age* in decades, thus we use 4 to calculate the respective marginal effect.] Thus, using this value we can interpret that, holding all other factors constant, an additional year of age decreases the likelihood of an individual joining the labor force by 9.3\%, supposing this individual is 40 years old. Another interpretation we can easily draw from this model is that a foreign worker ($foreign_i=0$) is 25\% more likely to be part of the labor force, *ceteris paribus*. In summary, we pick the values given by the regression output and interpret these as probabilities of the dependent variable being equal to 1. As an exercise, try to interpret the effect of the remaining variables on the target likelihood.

So far so good, right? Actually, no. For several observations, the predicted probability, i.e., $\hat{D}_i$ will be either greater than 1 or less than 0, which does not make logical sense. Even though this may not be verified for other values, there is no way of having a consistent estimation technique if it delivers probabilities that lie outside the reasonable bounds. Lastly, let us look at the scatter plot between $part_i$ and $income_i$, including the respective regression line with slope $\hat{\beta}_{educ} = -0.213$. Notice that the values are bounded between 0 and 1, but OLS estimation delivers a regression line that goes beyond 1 for some observations. This fact does not make sense and therefore the Linear Probability Model is not the best option for estimating models with binary dependent variables. We thus look at more appropriate techniques to deal with this issue.


```{r}
#| echo: false
#| label: fig-1
#| fig-cap: "A linear probability model."
#| column: page-right
#| fig-width: 6.5
#| fig-height: 3.5
#| message: false
#| warning: false

ggplot(SwissLabor, aes(x=income, y=part)) + 
  geom_point(alpha = 1/5, color = "#7997af", shape = 18, size = 1.8) + 
  geom_smooth(method = 'lm', se=FALSE, color = "#672c5e") +
  ggtitle("A Linear Probability Model")
```

# The Binomial Logit Model


One of the main issues associated with the Linear Probability Model is that some predicted probabilities end up being either greater than or lower than 0 or 1, the only accepted values for the dependent variable in models with *dummy* dependent regressors. Since a linear method such as OLS does not fulfill this basic requirements, we thus turn to *generalized* models, in which the dependent variable is no longer a linear function of the coefficients, independent variables, and error term. The first version we will study is the **Binomial Logit Model**, which assumes that the probability of success, i.e. $P(D_i = 1| x_i)$ takes the form of a *logistic* function $\cal L (\cdot)$. Thus, we write:


$$ P(D_i = 1 | x_i) = \mathcal{L}(\beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_kx_{ki}) $$
And $\cal L (\cdot)$ takes the following form:


$$ \mathcal{L} = \dfrac{e^{(\beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_kx_{ki})}}{1 + e^{(\beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_kx_{ki})}} $$


This functional form is the Cumulative Distribution Function (CDF) for a *logistically* distributed random variable, and it assures that the predicted probabilities for the dependent variable will lie between 0 and 1. Instead of estimating the model via OLS, as before, we use another estimator called *Maximum Likelihood*. Then, the *density* of $\cal L$, given the $\beta$ coefficients and the set of independent variables, becomes


$$ f(D_i | x_i, \beta_i) = \bigg[\mathcal{L}(\beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_kx_{ki})\bigg]^{D_i}\bigg[1- \mathcal{L}(\beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_kx_{ki})\bigg]^{1 -D_i}  $$

\noindent where $f(\cdot) = \cal L(\cdot)$ when $D_i=1$ and $f(\cdot) = 1 -\cal L(\cdot)$ when $D_i=0$.


## How to interpret Logit coefficients?

As with the LPM, the signs of the estimated coefficients are enough to assess whether a variable increases or decreases the likelihood of the dependent variable being equal to 1. However, in terms of their magnitudes, Logit coefficients are not directly interpretable. 

Fortunately, there are several alternatives to interpret marginal effects derived from Logit models. In this class, we will focus on one of the most common approaches: *average* marginal effects. These are basically

$$ \dfrac{\partial P(D_i=1|x_i)}{\partial x_{ij}} = \dfrac{\partial \mathcal{L (\cdot)}}{\partial x_{ij}} = \dfrac{\sum_{i=1}^{n} f(\hat{\beta}_0 + \hat{\beta}_1x_1 + \hat{\beta}_2x_2 + ... + \hat{\beta}_kx_k) }{n} \cdot \hat{\beta}_j  $$
\noindent where $f(\cdot)$ is the probability density function evaluated at the estimated coefficient values, multiplied by the independent variables. This partial derivative is the mean effect of a one-unit change in the $j^{th}$ independent variable. Its application is easily implemented in $\textsf{R}$, which will be studied in the applied lecture.



## An example

Now that we know that there is a different estimator for binary dependent variable models, let us compute the same model as before, this time using Logit regression. Table 2 illustrates the results.



```{r}
#| echo: false
#| message: false
#| warning: false
#| results: asis


stargazer(logit, title="A Binomial Logit Model", header=FALSE, no.space = TRUE)

```



First, notice that the signs are the same as the ones delivered by the Linear Probability Model. In addition, evaluating the statistical significance of Logit coefficients is done in the same way as we are used to, using either the test statistic or the p-value. Observe that the table does not report any goodness-of-fit measure. For Logit and Probit models (which will be seen in the next section), these measures are no longer valid. However, we will look at a different way of assessing model fit later in these notes, which will use the *Log-Likelihood* informed by the regression output table. Lastly, the *Akaike Information Criterion* is a measure used for model comparison. Here, we will not study this criterion in detail.

The coefficients' magnitudes cannot be directly interpreted, as stated before. Thus, we can compute average marginal effects. Details on these computations will be provided in the applied lecture, but let us interpret the effect of *income* on the likelihood of an individual participating in the labor force.

